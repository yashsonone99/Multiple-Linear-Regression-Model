# -*- coding: utf-8 -*-
"""MLR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oEZZs5XmMPClp1dRaU14AImb7COvpbLo
"""

# Import basic data handling, visualization, and ML libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Load dataset
df = pd.read_csv("/content/ToyotaCorolla - MLR.csv")

# Display basic info
print("âœ… Dataset loaded successfully!")
print("Shape:", df.shape)
df.head()

# Display column names and data types
print("Columns:\n", df.columns.tolist())
print("\nData Info:")
df.info()

# Check for missing values
print("\nMissing Values:\n", df.isnull().sum())

# Statistical summary
df.describe()

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Separate numeric and categorical columns
num_cols = df.select_dtypes(include=['number']).columns
cat_cols = df.select_dtypes(exclude=['number']).columns

# Handle missing values for numeric columns with median
df[num_cols] = df[num_cols].fillna(df[num_cols].median())

# Handle missing values for categorical columns with mode
for col in cat_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

# Verify results
print("\nâœ… Missing values handled successfully!")
print(df.isnull().sum())

# ðŸ“Š STEP 6 â€” EXPLORATORY DATA ANALYSIS (EDA)
# Objective: Understand data distributions and relationships

# Correlation heatmap (only numeric columns)
plt.figure(figsize=(10,6))
sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap of Numeric Variables")
plt.show()

# Pairplot of key numeric variables vs Price
sns.pairplot(df[['Price','Age_08_04','KM','HP','cc','Doors','Weight']])
plt.suptitle("Pairwise Relationships between Key Variables", y=1.02)
plt.show()

# Distribution of important numerical variables
for col in ['Price','Age_08_04','KM','HP','cc','Weight']:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col], kde=True, color='teal')
    plt.title(f"Distribution of {col}")
    plt.show()

# Boxplots to detect outliers
for col in ['Price','Age_08_04','KM','HP','cc','Weight']:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df[col], color='orange')
    plt.title(f"Boxplot for {col}")
    plt.show()

# Select relevant independent variables based on correlation & domain logic
X = df[['Age_08_04', 'KM', 'Fuel_Type', 'HP', 'Automatic', 'cc', 'Doors', 'Weight']]
y = df['Price']

# Identify numeric and categorical columns for preprocessing
num_cols = ['Age_08_04', 'KM', 'HP', 'cc', 'Doors', 'Weight']
cat_cols = ['Fuel_Type', 'Automatic']

print("âœ… Selected Features:", X.columns.tolist())

# â€” DATA PREPROCESSING PIPELINE

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Build preprocessing pipeline
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(drop='first'), cat_cols)
])

print("âœ… Preprocessing pipeline created successfully.")

# TRAIN / TEST SPLIT

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Training Shape :", X_train.shape)
print("Testing Shape  :", X_test.shape)

# MULTIPLE LINEAR REGRESSION MODEL

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np

linear_model = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])
linear_model.fit(X_train, y_train)

# Predictions
y_pred_lin = linear_model.predict(X_test)

# Metrics
r2_lin = r2_score(y_test, y_pred_lin)
rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))
mae_lin = mean_absolute_error(y_test, y_pred_lin)

print(f"ðŸ“Š Linear Regression Results:\nRÂ² = {r2_lin:.3f}\nRMSE = {rmse_lin:.2f}\nMAE = {mae_lin:.2f}")

# RIDGE REGRESSION (L2 Regularization)

from sklearn.linear_model import Ridge

ridge_model = Pipeline([
    ('preprocessor', preprocessor),
    ('ridge', Ridge(alpha=1.0))
])
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_test)

r2_ridge = r2_score(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)

print(f"ðŸ“Š Ridge Regression Results:\nRÂ² = {r2_ridge:.3f}\nRMSE = {rmse_ridge:.2f}\nMAE = {mae_ridge:.2f}")

# LASSO REGRESSION (L1 Regularization)

from sklearn.linear_model import Lasso

lasso_model = Pipeline([
    ('preprocessor', preprocessor),
    ('lasso', Lasso(alpha=0.1))
])
lasso_model.fit(X_train, y_train)
y_pred_lasso = lasso_model.predict(X_test)

r2_lasso = r2_score(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)

print(f"ðŸ“Š Lasso Regression Results:\nRÂ² = {r2_lasso:.3f}\nRMSE = {rmse_lasso:.2f}\nMAE = {mae_lasso:.2f}")

# MODEL PERFORMANCE COMPARISON

comparison = pd.DataFrame({
    'Model': ['Linear', 'Ridge', 'Lasso'],
    'RÂ² Score': [r2_lin, r2_ridge, r2_lasso],
    'RMSE': [rmse_lin, rmse_ridge, rmse_lasso],
    'MAE': [mae_lin, mae_ridge, mae_lasso]
})

print("ðŸ“Š Model Comparison Table:")
display(comparison)

# VISUALIZATION: ACTUAL VS PREDICTED

plt.figure(figsize=(6,4))
sns.scatterplot(x=y_test, y=y_pred_lin, color='royalblue', alpha=0.6)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Prices (Linear Regression)")
plt.show()

# INSIGHTS AND CONCLUSION

print("""
âœ” Model fits data well with a good RÂ² score and acceptable RMSE.
âœ” Ridge and Lasso provide slightly improved generalization over basic Linear Regression.
âœ” Age and KM show strong negative impact on price; HP and Weight have positive impact.
âœ” Regularization reduces overfitting and improves interpretability.
âœ” Final model can be deployed for real-world car price prediction.
""")

# SAVE TRAINED MODEL

import joblib
joblib.dump(linear_model, "toyota_mlr_model.pkl")
print("âœ… Model saved successfully as toyota_mlr_model.pkl")

